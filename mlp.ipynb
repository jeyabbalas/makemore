{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Makemore - MLP\n",
    "\n",
    "![MLP architecture](/images/mlp_bengio03.png)\n",
    "\n",
    "- This implementation is based on [(Bengio et al., 2003)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). While it was not the first paper to propose MLP to predict the next token, it was one of the most influential.\n",
    "    - **Feature encoding**: embed each of the 17,000 words in the vocabulary into a 30-D feature space. Initially, the words are embedded randomly. Then learn the embedding using backpropagation. During training, words with similar meaning end up in a similar part of the space and words with dissimilar meanings move apart. The embedding look-up table `C` is a `17,000 x 30` matrix. Each row is a 30-D vector representing a word in the vocabulary.\n",
    "    - **Model**: an MLP to predict the next word in the sequence, given 3 previous words. Each word is a 30-D vector, so the input is a 90-D vector. Each tanh-neuron in the hidden layer is fully-connected to the 90 input variables. The output layer has 17,000 neurons, fully-connected to the hidden layer neurons.\n",
    "    - **Loss**: maximize the log-likelihood of the training dataset or minimize the average negative log-likelihood.\n",
    "    - **Optimization**: both the embedding and the network parameters are optimized using back-propagation.\n",
    "- **Why does it work?**: even if during inference, you encounter sentences you have never seen in the training dataset, you can still look for semantically similar sentences in the training dataset and use them to predict the next word. This is why good embeddings are important.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
